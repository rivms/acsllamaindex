{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure Cognitive Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Example\n",
    "\n",
    "In this basic example, we take  a Paul Graham essay, split it into chunks, embed it using an OpenAI embedding model, load it into an Azure Cognitive Search index, and then query it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "# logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "# logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install openai\n",
    "!{sys.executable} -m pip install llama-index\n",
    "!{sys.executable} -m pip install azure-search-documents==11.4.0b8\n",
    "!{sys.executable} -m pip install azure-identity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get AAD Auth Token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default Azure AAD credential is used. Ensure \"az login\" has been run and the logged in user has access to the Azure OpenAI and ACS instances used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# Request credential\n",
    "default_credential = DefaultAzureCredential()\n",
    "token = default_credential.get_token(\"https://cognitiveservices.azure.com/.default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure Azure OpenAI LLM and Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aoai_base = \"https://demoaoai002.openai.azure.com/\"\n",
    "azure_kwargs = {\n",
    "    \"api_type\": \"azure_ad\",\n",
    "    \"api_version\": \"2023-03-15-preview\",\n",
    "    \"api_base\": aoai_base,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step assumes a deployment name of \"text-davinci-003\", update this name if you have used a different name for your deloyment of the \"text-davinci-003\" model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import AzureOpenAI\n",
    "\n",
    "llm = AzureOpenAI(\n",
    "    temperature=0.9,\n",
    "    deployment_name=\"text-davinci-003\", # Update deployment name if necessary\n",
    "    model_name=\"text-davinci-003\",\n",
    "    openai_api_version=azure_kwargs[\"api_version\"],\n",
    "    openai_api_key=token.token, # Authenticate using AAD token\n",
    "    model_kwargs=azure_kwargs,\n",
    ")\n",
    "\n",
    "from llama_index import LLMPredictor\n",
    "\n",
    "# define LLM\n",
    "llm_predictor = LLMPredictor(llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step assumes a deployment name of \"text-embedding-ada-002\", update this name if you have used a different name for your deloyment of the \"text-embedding-ada-002\" model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from llama_index import LangchainEmbedding\n",
    "\n",
    "# load in AOAI embedding model from langchain\n",
    "oai_embeddings = OpenAIEmbeddings(\n",
    "    deployment=\"text-embedding-ada-002\", # Update deployment name if necessary\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    openai_api_key=token.token, # Authenticate using AAD token\n",
    "    openai_api_base=azure_kwargs[\"api_base\"],\n",
    "    openai_api_type=azure_kwargs[\"api_type\"],\n",
    "    openai_api_version=azure_kwargs[\"api_version\"],\n",
    "    chunk_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up Azure Cognitive Search\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "import getpass\n",
    "\n",
    "search_service_name = getpass.getpass(\"Azure Cognitive Search Service Name\")\n",
    "\n",
    "key = getpass.getpass(\"Azure Cognitive Search Key\")\n",
    "\n",
    "cognitive_search_credential = AzureKeyCredential(key)\n",
    "\n",
    "service_endpoint = f\"https://{search_service_name}.search.windows.net\"\n",
    "\n",
    "# Index name to use\n",
    "index_name = \"quickstart\"\n",
    "\n",
    "# Use index client to demonstrate creating an index\n",
    "index_client = SearchIndexClient(\n",
    "    endpoint=service_endpoint,\n",
    "    credential=cognitive_search_credential, \n",
    ")\n",
    "\n",
    "# Use search client to demonstrate using an existing index\n",
    "search_client = SearchClient(\n",
    "    endpoint=service_endpoint,\n",
    "    index_name=index_name,\n",
    "    credential=cognitive_search_credential, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Index (if it does not exist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstrates creating a vector index named quickstart01 if one doesn't exist. The index has the following fields:\n",
    "- id (Edm.String)\n",
    "- content (Edm.String)\n",
    "- embedding (Edm.SingleCollection)\n",
    "- li_jsonMetadata (Edm.String)\n",
    "- li_doc_id (Edm.String)\n",
    "- author (Edm.String)\n",
    "- theme (Edm.String)\n",
    "- director (Edm.String)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents import SearchClient\n",
    "from llama_index.vector_stores import CognitiveSearchVectorStore\n",
    "from llama_index.vector_stores.cogsearch import (\n",
    "    IndexManagement,\n",
    "    MetadataIndexFieldType,\n",
    "    CognitiveSearchVectorStore,\n",
    ")\n",
    "\n",
    "# Example of a complex mapping, metadata field 'theme' is mapped to a differently name index field 'topic' with its type explicitly set\n",
    "metadata_fields = {\n",
    "    \"author\": \"author\",\n",
    "    \"theme\": (\"topic\", MetadataIndexFieldType.STRING),\n",
    "    \"director\": \"director\",\n",
    "}\n",
    "\n",
    "# A simplified metadata specification is available if all metadata and index fields are similarly named\n",
    "# metadata_fields = {\"author\", \"theme\", \"director\"}\n",
    "\n",
    "\n",
    "vector_store = CognitiveSearchVectorStore(\n",
    "    search_or_index_client=index_client,\n",
    "    index_name=index_name,\n",
    "    filterable_metadata_field_keys=metadata_fields,\n",
    "    index_management=IndexManagement.CREATE_IF_NOT_EXISTS,\n",
    "    id_field_key=\"id\",\n",
    "    chunk_field_key=\"content\",\n",
    "    embedding_field_key=\"embedding\",\n",
    "    metadata_string_field_key=\"li_jsonMetadata\",\n",
    "    doc_id_field_key=\"li_doc_id\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define embedding function\n",
    "from llama_index.embeddings import OpenAIEmbedding\n",
    "from llama_index import (\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    ServiceContext,\n",
    "    VectorStoreIndex,\n",
    ")\n",
    "\n",
    "\n",
    "# load documents\n",
    "documents = SimpleDirectoryReader(\n",
    "    \"./doc_samples/paul_graham_essay/data\"\n",
    ").load_data()\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "service_context = ServiceContext.from_defaults(embed_model=oai_embeddings, llm_predictor=llm_predictor)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context, service_context=service_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Data\n",
    "query_engine = index.as_query_engine(similarity_top_k=3)\n",
    "response = query_engine.query(\"What did the author do growing up?\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\n",
    "    \"What did the author learn?\",\n",
    ")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Existing Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores import CognitiveSearchVectorStore\n",
    "from llama_index.vector_stores.cogsearch import (\n",
    "    IndexManagement,\n",
    "    MetadataIndexFieldType,\n",
    "    CognitiveSearchVectorStore,\n",
    ")\n",
    "\n",
    "\n",
    "index_name = \"quickstart\"\n",
    "\n",
    "metadata_fields = {\n",
    "    \"author\": \"author\",\n",
    "    \"theme\": (\"topic\", MetadataIndexFieldType.STRING),\n",
    "    \"director\": \"director\",\n",
    "}\n",
    "vector_store = CognitiveSearchVectorStore(\n",
    "    search_or_index_client=search_client,\n",
    "    filterable_metadata_field_keys=metadata_fields,\n",
    "    index_management=IndexManagement.NO_VALIDATION,\n",
    "    id_field_key=\"id\",\n",
    "    chunk_field_key=\"content\",\n",
    "    embedding_field_key=\"embedding\",\n",
    "    metadata_string_field_key=\"li_jsonMetadata\",\n",
    "    doc_id_field_key=\"li_doc_id\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define embedding function\n",
    "from llama_index.embeddings import OpenAIEmbedding\n",
    "from llama_index import (\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    ServiceContext,\n",
    "    VectorStoreIndex,\n",
    ")\n",
    "\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "service_context = ServiceContext.from_defaults(embed_model=oai_embeddings, llm_predictor=llm_predictor)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    [], storage_context=storage_context, service_context=service_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What was a hard moment for the author?\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"Who is the author?\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "query_engine = index.as_query_engine(streaming=True)\n",
    "response = query_engine.query(\"What happened at interleaf?\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "token_count = 0\n",
    "for token in response.response_gen:\n",
    "    print(token, end=\"\")\n",
    "    token_count += 1\n",
    "\n",
    "time_elapsed = time.time() - start_time\n",
    "tokens_per_second = token_count / time_elapsed\n",
    "\n",
    "print(f\"\\n\\nStreamed output at {tokens_per_second} tokens/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding a document to existing index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"What colour is the sky?\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import Document\n",
    "\n",
    "index.insert_nodes([Document(text=\"The sky is indigo today\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"What colour is the sky?\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.schema import TextNode\n",
    "\n",
    "nodes = [\n",
    "    TextNode(\n",
    "        text=\"The Shawshank Redemption\",\n",
    "        metadata={\n",
    "            \"author\": \"Stephen King\",\n",
    "            \"theme\": \"Friendship\",\n",
    "        },\n",
    "    ),\n",
    "    TextNode(\n",
    "        text=\"The Godfather\",\n",
    "        metadata={\n",
    "            \"director\": \"Francis Ford Coppola\",\n",
    "            \"theme\": \"Mafia\",\n",
    "        },\n",
    "    ),\n",
    "    TextNode(\n",
    "        text=\"Inception\",\n",
    "        metadata={\n",
    "            \"director\": \"Christopher Nolan\",\n",
    "        },\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.insert_nodes(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.types import ExactMatchFilter, MetadataFilters\n",
    "\n",
    "\n",
    "filters = MetadataFilters(filters=[ExactMatchFilter(key=\"theme\", value=\"Mafia\")])\n",
    "\n",
    "retriever = index.as_retriever(filters=filters)\n",
    "retriever.retrieve(\"What is inception about?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaindextest01",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
